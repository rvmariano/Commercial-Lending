{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "056a0f20-af7f-4047-a968-7cec851ee3fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Start of Incremental Load\n",
    "Start of incremental loads of every table in the schema. It runs everyday, but keeps only last available date + month ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0572037a-8987-4419-88fc-364d99f82d4f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Checks if there is difference between last loaded and current data"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Check if there is a difference between the last loaded and current data in a generic incremental ETL pipeline\n",
    "# This is a common pattern for incremental data loads in data engineering\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Replace 'mydb.incremental_table' and 'mydb.current_table' with your actual table names\n",
    "day_delta_df = spark.sql(\"\"\"\n",
    "  SELECT \n",
    "      i.load_date,\n",
    "      SUM(i.amount) - b.total_amount AS day_delta\n",
    "  FROM mydb.incremental_table i\n",
    "  JOIN (\n",
    "      SELECT SUM(c.amount) AS total_amount\n",
    "      FROM mydb.current_table c\n",
    "  ) b\n",
    "  GROUP BY i.load_date, b.total_amount\n",
    "  ORDER BY i.load_date DESC\n",
    "  LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "# Get the difference value\n",
    "try:\n",
    "    day_delta = day_delta_df.select(\"day_delta\").collect()[0][0]\n",
    "except IndexError:\n",
    "    day_delta = None\n",
    "\n",
    "current_weekday = datetime.now().strftime('%A')\n",
    "\n",
    "# If there is no difference and today is a weekday, stop the script\n",
    "if (day_delta is None or day_delta == 0) and current_weekday not in ['Saturday', 'Sunday']:\n",
    "    sys.exit(\"No new data to process. Aborting script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59b5ac4-83d2-49af-892e-40c57b8c38e8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dealer incremental load"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Incremental load for a dimension table in a generic data warehouse\n",
    "# This pattern loads new records and unions with previous incremental data, keeping only the latest and month-end records\n",
    "\n",
    "# Replace 'source_db.generic_table' and 'analytics_db.incremental_table' with your own table names\n",
    "\n",
    "generic_df = spark.sql(\"\"\"\n",
    "  SELECT *, current_date() AS run_date\n",
    "  FROM source_db.generic_table\n",
    "\"\"\")\n",
    "\n",
    "# Load previous incremental data, excluding today's records and keeping only month-ends\n",
    "prev_incremental_data = spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM analytics_db.incremental_table\n",
    "  WHERE run_date <> current_date() AND date_format(run_date, 'dd') = '01'\n",
    "\"\"\")\n",
    "\n",
    "# Union new and previous data\n",
    "incremental_data = generic_df.unionByName(prev_incremental_data)\n",
    "\n",
    "# Overwrite the incremental table with the new data\n",
    "incremental_data.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.incremental_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6f810a-7dc8-446c-bf18-d7134ae6b4d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Credit Limit incremental load"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Incremental load for a fact table in a generic data warehouse\n",
    "# Replace 'source_db.fact_table' and 'analytics_db.incremental_table' with your own table names\n",
    "\n",
    "fact_table_df = spark.sql(\"\"\"\n",
    "  SELECT *, current_date() AS run_date\n",
    "  FROM source_db.fact_table\n",
    "\"\"\")\n",
    "\n",
    "prev_incremental_table = spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM analytics_db.incremental_table\n",
    "  WHERE run_date <> current_date() AND date_format(run_date, 'dd') = '01'\n",
    "\"\"\")\n",
    "\n",
    "incremental_table = fact_table_df.unionByName(prev_incremental_table)\n",
    "\n",
    "incremental_table.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.incremental_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a099763-6746-4a69-829a-5c5248f72f80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Behavior Classification incremental load"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Incremental load for the Behavior Classification dimension table\n",
    "# Replace 'source_db.generic_behavior' and 'analytics_db.incremental_behavior' with your own table names\n",
    "\n",
    "generic_behavior_df = spark.sql(\"\"\"\n",
    "  SELECT *, current_date() AS run_date\n",
    "  FROM source_db.generic_behavior\n",
    "\"\"\")\n",
    "\n",
    "prev_incremental_behavior = spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM analytics_db.incremental_behavior\n",
    "  WHERE run_date <> current_date() AND date_format(run_date, 'dd') = '01'\n",
    "\"\"\")\n",
    "\n",
    "incremental_behavior = generic_behavior_df.unionByName(prev_incremental_behavior)\n",
    "\n",
    "incremental_behavior.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.incremental_behavior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bb471f6-c472-4ceb-ab72-a6d8de3a16c2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Equipment incremental load"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Incremental load for the Equipment dimension table\n",
    "# Replace 'source_db.equipment' and 'analytics_db.incremental_equipment' with your own table names\n",
    "\n",
    "equipment_df = spark.sql(\"\"\"\n",
    "  SELECT *, current_date() AS run_date\n",
    "  FROM source_db.equipment\n",
    "\"\"\")\n",
    "\n",
    "prev_incremental_equipment = spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM analytics_db.incremental_equipment\n",
    "  WHERE run_date <> current_date() AND date_format(run_date, 'dd') = '01'\n",
    "\"\"\")\n",
    "\n",
    "incremental_equipment = equipment_df.unionByName(prev_incremental_equipment)\n",
    "\n",
    "incremental_equipment.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.incremental_equipment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053ce80f-4535-46d3-9b07-87a5aa1bdbfb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Product incremental load"
    }
   },
   "outputs": [],
   "source": [
    "# Incremental load for the Product dimension table\n",
    "# Replace 'source_db.product' and 'analytics_db.incremental_product' with your own table names\n",
    "\n",
    "product_df = spark.sql(\"\"\"\n",
    "  SELECT *, current_date() AS run_date\n",
    "  FROM source_db.product\n",
    "\"\"\")\n",
    "\n",
    "prev_incremental_product = spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM analytics_db.incremental_product\n",
    "  WHERE run_date <> current_date() AND date_format(run_date, 'dd') = '01'\n",
    "\"\"\")\n",
    "\n",
    "incremental_product = product_df.unionByName(prev_incremental_product)\n",
    "\n",
    "incremental_product.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.incremental_product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca81c7ae-23d9-47f5-b8db-7483c213eb11",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Equipment Dates incremental load"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Incremental load for the Equipment Dates dimension table\n",
    "# Replace 'source_db.equipment_dates' and 'analytics_db.incremental_equipment_dates' with your own table names\n",
    "\n",
    "equipment_dates_df = spark.sql(\"\"\"\n",
    "  SELECT *, current_date() AS run_date\n",
    "  FROM source_db.equipment_dates\n",
    "\"\"\")\n",
    "\n",
    "prev_incremental_equipment_dates = spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM analytics_db.incremental_equipment_dates\n",
    "  WHERE run_date <> current_date() AND date_format(run_date, 'dd') = '01'\n",
    "\"\"\")\n",
    "\n",
    "incremental_equipment_dates = equipment_dates_df.unionByName(prev_incremental_equipment_dates)\n",
    "\n",
    "incremental_equipment_dates.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.incremental_equipment_dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ca0c82-fdd8-4e00-bf2f-bcd0d2c9bc29",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fact Recv_Root incremental load"
    }
   },
   "outputs": [],
   "source": [
    "# Incremental load for a Fact table with joins in a generic data warehouse\n",
    "# This example demonstrates joining a fact table with calculation and inventory tables, then performing an incremental load pattern.\n",
    "# Replace all table and column names with your own as needed.\n",
    "\n",
    "# Step 1: Load today's fact data, joining with calculation and inventory tables\n",
    "fact_today_df = spark.sql(\"\"\"\n",
    "  SELECT\n",
    "    f.*, \n",
    "    current_date() AS run_date,\n",
    "    inv.inventory_id,\n",
    "    inv.inventory_flag,\n",
    "    calc.product_type,\n",
    "    calc.interest_code\n",
    "  FROM source_db.fact_table f\n",
    "  INNER JOIN source_db.fact_table_calc calc ON f.fact_key = calc.fact_key\n",
    "  LEFT OUTER JOIN source_db.fact_inventory_link inv ON f.fact_key = inv.fact_key\n",
    "\"\"\")\n",
    "\n",
    "# Step 2: Load previous incremental data, excluding today's records and keeping only month-ends\n",
    "prev_incremental_fact = spark.sql(\"\"\"\n",
    "  SELECT *\n",
    "  FROM analytics_db.incremental_fact_table\n",
    "  WHERE run_date <> current_date() AND date_format(run_date, 'dd') = '01'\n",
    "\"\"\")\n",
    "\n",
    "# Step 3: Union today's and previous data\n",
    "incremental_fact = fact_today_df.unionByName(prev_incremental_fact)\n",
    "\n",
    "# Step 4: Overwrite the incremental fact table with the new data\n",
    "incremental_fact.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.incremental_fact_table\")\n",
    "\n",
    "# Note: Replace 'source_db' and 'analytics_db' with your own database/schema names.\n",
    "#       Replace column names with those relevant to your data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90a92d1e-a911-4d84-8d9a-175689498ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "End of incremental loads and start of Fact Table transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514fe7fb-390b-4f46-abce-a4fee5a30cdc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filtering and creation of key fields"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering and creation of key fields for the fact table\n",
    "# This cell demonstrates how to join incremental fact and dimension tables, create key fields, and apply business logic in a generic data warehouse\n",
    "\n",
    "T1_Fact = spark.sql('''\n",
    "  select \n",
    "    date_add(f.run_date, -1)                                      as run_date,\n",
    "    f.fact_key                                                    as fact_id,\n",
    "    f.credit_line_id                                              as credit_line_id,\n",
    "    f.invoice_id                                                  as invoice_id,\n",
    "    d.dealer_id                                                   as dealer_id,\n",
    "    f.company_id                                                  as company_id,\n",
    "    f.receivable_date                                             as receivable_date,\n",
    "    f.unpaid_balance                                              as unpaid_balance,\n",
    "    f.past_due_amount                                             as past_due_amount,\n",
    "    f.owner                                                       as owner,\n",
    "    f.ag_percentage                                               as ag_percentage,\n",
    "    f.ce_percentage                                               as ce_percentage,\n",
    "    f.last_interest_date                                          as last_interest_date,\n",
    "    f.current_interest_date                                       as current_interest_date,\n",
    "    f.current_maturity_date                                       as current_maturity_date,\n",
    "    f.maturity_date                                               as maturity_date,\n",
    "    f.interest_bearing_code                                       as interest_bearing_code,\n",
    "    f.major_flag                                                  as major_flag,\n",
    "    f.equipment_id                                                as equipment_id,\n",
    "    f.suffix                                                      as suffix,\n",
    "    f.product_brand                                               as product_brand,\n",
    "    d.country                                                     as country,\n",
    "    f.bearing_type                                                as bearing_type,\n",
    "    e.new_or_used_indicator                                       as new_or_used_indicator,\n",
    "    -- Example business logic for aging comparison\n",
    "    if(\n",
    "      (f.unpaid_balance > 0 or (e.current_status_code = '4990' and f.credit_line_id = '011') or (e.current_status_code = '3000'))\n",
    "      and d.termination_code not in ('8', '9')\n",
    "      and f.credit_line_id not in ('13', '15')\n",
    "      and e.current_status_code not in ('5000','5500','6000','6100','7000','9000'),\n",
    "      1, 0\n",
    "    ) as aging_comparison\n",
    "  from analytics_db.incremental_fact_table f\n",
    "    inner join analytics_db.incremental_table d \n",
    "      on f.dealer_id = d.dealer_id and f.company_id = d.company_id and f.run_date = d.run_date\n",
    "    left join analytics_db.incremental_equipment e \n",
    "      on e.equipment_id = f.equipment_id and f.run_date = e.run_date\n",
    "  where \n",
    "    f.unpaid_balance <> 0\n",
    "    and f.owner = 'C'\n",
    "    and (f.major_flag is null or f.major_flag = 'Y')\n",
    "    and d.brand in ('CASE', 'FPT', 'MFS', 'n-CAP', 'n-CASE', 'n-NH', 'NH')\n",
    "''')\n",
    "\n",
    "T1_Fact.createOrReplaceTempView(\"T1_Fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a0c4280-3b80-4813-8a28-cab1cc37d4e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cross Joins to have all possible combinations on Fact table, so Plan table can connect without data loss"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Get distinct values for each column from the generic fact table\n",
    "distinct_run_date = Generic_Fact_Table.select(\"run_date\").distinct()  # Distinct run dates\n",
    "distinct_product_brand = Generic_Fact_Table.select(\"product_brand\").distinct()  # Distinct product brands\n",
    "distinct_country = Generic_Fact_Table.select(\"country\").distinct()  # Distinct countries\n",
    "distinct_bearing_type = Generic_Fact_Table.select(\"bearing_type\").distinct()  # Distinct bearing types\n",
    "distinct_new_or_used = Generic_Fact_Table.select(\"new_or_used_indicator\").distinct()  # Distinct new or used indicators\n",
    "\n",
    "# Cross join the distinct values to get all possible combinations\n",
    "AllCombinations = (\n",
    "    distinct_run_date\n",
    "    .crossJoin(distinct_product_brand)\n",
    "    .crossJoin(distinct_country)\n",
    "    .crossJoin(distinct_bearing_type)\n",
    "    .crossJoin(distinct_new_or_used)\n",
    ")\n",
    "\n",
    "# Register as temporary view for further processing\n",
    "AllCombinations.createOrReplaceTempView(\"AllCombinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60570f58-527c-440c-9f70-d014135384e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fact table transformations"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, when, col\n",
    "\n",
    "# Fact table transformations: join with all combinations, create keys, and add derived columns\n",
    "Complete_Fact = spark.sql('''\n",
    "  select *,\n",
    "    run_date||dealer_id as dealer_key,\n",
    "    run_date||equipment_id as equipment_key,\n",
    "    year_month||product_brand||country||bearing_type||new_or_used_indicator as plan_id,\n",
    "    year_month||currency as fx_id  \n",
    "  from(\n",
    "    select\n",
    "      a.bearing_type,\n",
    "      a.run_date,\n",
    "      a.product_brand,\n",
    "      a.country,\n",
    "      a.new_or_used_indicator,\n",
    "      date_format(a.run_date, 'MMM') as run_month,    \n",
    "      to_number(date_format(a.run_date, 'M'), '99') as run_month_id,  \n",
    "      date_format(a.run_date, 'yyyyMM') as year_month,\n",
    "      date_format(a.run_date, 'yyyy') as run_year,      \n",
    "      fact_id,\n",
    "      cast(credit_line_id as int) as credit_line_id,\n",
    "      invoice_id,\n",
    "      dealer_id,\n",
    "      company_id,\n",
    "      receivable_date,\n",
    "      unpaid_balance,\n",
    "      past_due_amount,\n",
    "      owner,\n",
    "      ag_percentage,\n",
    "      ce_percentage,\n",
    "      last_interest_date,\n",
    "      current_interest_date,\n",
    "      current_maturity_date,\n",
    "      maturity_date,\n",
    "      interest_bearing_code,\n",
    "      major_flag,\n",
    "      equipment_id,\n",
    "      suffix,\n",
    "      aging_comparison,\n",
    "      datediff(a.run_date, maturity_date) as past_due_days,\n",
    "      datediff(last_day(a.run_date), maturity_date) as rounded_past_due_days,\n",
    "      CASE WHEN suffix in ('A8', 'R8') THEN 'Yes' ELSE 'No' END as dealer_credits,\n",
    "      CASE\n",
    "        WHEN datediff(a.run_date, receivable_date) between 0 and 30 THEN '0-30 Days'\n",
    "        WHEN datediff(a.run_date, receivable_date) between 31 and 60 THEN '31-60 Days'\n",
    "        WHEN datediff(a.run_date, receivable_date) between 61 and 90 THEN '61-90 Days'\n",
    "        WHEN datediff(a.run_date, receivable_date) between 91 and 180 THEN '91-180 Days'\n",
    "        WHEN datediff(a.run_date, receivable_date) between 181 and 365 THEN '181-365 Days'\n",
    "        WHEN datediff(a.run_date, receivable_date) between 366 and 730 THEN '366-730 Days'\n",
    "        WHEN datediff(a.run_date, receivable_date) > 730 THEN '731+ Days'\n",
    "      END as aging_buckets,\n",
    "      CASE\n",
    "        WHEN datediff(a.run_date, receivable_date) between 0 and 365 THEN '0-365 Days'\n",
    "        WHEN datediff(a.run_date, receivable_date) between 366 and 730 THEN '366-730 Days'\n",
    "        WHEN datediff(a.run_date, receivable_date) > 730 THEN '731+ Days'\n",
    "      END as short_aging_buckets,\n",
    "      CASE \n",
    "        WHEN a.product_brand = 'Case IH' THEN 'AG'\n",
    "        WHEN a.product_brand = 'Case CE' THEN 'CE'\n",
    "        WHEN a.product_brand = 'NH AG' THEN 'AG'\n",
    "        WHEN a.product_brand = 'NH CE' THEN 'CE' \n",
    "        WHEN a.product_brand = 'ALLIED' THEN \n",
    "          CASE \n",
    "            WHEN ag_percentage = 1 THEN 'AG'\n",
    "            WHEN ce_percentage = 1 THEN 'CE'\n",
    "            ELSE 'FPT'\n",
    "          END\n",
    "        WHEN a.product_brand = 'FPT' THEN 'FPT'\n",
    "      END as product_ind,\n",
    "      CASE WHEN a.product_brand = 'ALLIED' THEN 'ALLIED' ELSE 'CAPTIVE' END as captive_allied, \n",
    "      if(a.new_or_used_indicator = 'N', 'New Units', if(a.new_or_used_indicator = 'U', 'Used Units', 'Other')) as new_or_used_desc,\n",
    "      if(a.country = 'CA', 'CAD', 'USD') as currency  \n",
    "    from AllCombinations a\n",
    "    left join T1_Fact t\n",
    "      on a.run_date||a.product_brand||a.country||a.bearing_type||a.new_or_used_indicator \n",
    "      =  t.run_date||t.product_brand||t.country||t.bearing_type||t.new_or_used_indicator\n",
    "  )\n",
    "''')\n",
    "\n",
    "# Add sequential IDs to null fact_id values\n",
    "Complete_Fact = Complete_Fact.withColumn(\n",
    "    \"fact_id\",\n",
    "    when(col(\"fact_id\").isNull(), monotonically_increasing_id()).otherwise(col(\"fact_id\"))\n",
    ")\n",
    "\n",
    "# Save the transformed fact table to the analytics database\n",
    "Complete_Fact.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.complete_fact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c16dc12-8635-4fcd-b024-14205e4be296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "End of transformations. Start of loading to final tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a31a7a2-97ed-4087-9a0a-9cc15a20d8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create the final fact table, ensuring all dealers with credit limits are included\n",
    "FACT_Fact = spark.sql('''\n",
    "  select\n",
    "    fact_id||run_date as key_fact,  -- Unique key for the fact record\n",
    "    dealer_key,                     -- Key for the dealer\n",
    "    equipment_key,                  -- Key for the equipment\n",
    "    plan_id,                        -- Key for the plan\n",
    "    fx_id,                          -- Key for the foreign exchange\n",
    "    run_date,                       -- Date of the run\n",
    "    year_month,                     -- Year and month of the run\n",
    "    credit_line_id,                 -- Key for the credit line\n",
    "    invoice_id,                     -- Key for the invoice\n",
    "    dealer_id,                      -- ID of the dealer\n",
    "    company_id,                     -- ID of the company\n",
    "    unpaid_balance,                 -- Amount of unpaid balance\n",
    "    past_due_amount,                -- Amount past due\n",
    "    major_flag,                     -- Indicator for major status\n",
    "    equipment_id                    -- ID of the equipment\n",
    "  from generic_db.generic_fact_table  \n",
    "''')\n",
    "\n",
    "# Ensure all dealers with credit limits are present in the fact table\n",
    "all_dealers = spark.sql('''\n",
    "  select distinct\n",
    "    date_add(d.run_date, -1) as run_date,  -- Adjusted run date for consistency\n",
    "    d.dealer_id as dealer_id,               -- ID of the dealer\n",
    "    date_add(d.run_date, -1)||d.dealer_id as dealer_key  -- Unique key for the dealer\n",
    "  from generic_db.generic_incremental_dealer d\n",
    "  left join \n",
    "      (select \n",
    "        dealer_key, \n",
    "        date_add(run_date, -1) as run_date,\n",
    "        sum(credit_limit_amt) as credit_limit_amt  -- Total credit limit amount\n",
    "      from generic_db.generic_incremental_creditlimit\n",
    "      group by dealer_key, run_date) cr \n",
    "    on d.dealer_key = cr.dealer_key and date_add(d.run_date, -1) = cr.run_date\n",
    "  where d.dealer_id is not null and cr.credit_limit_amt is not null and cr.credit_limit_amt <> 0\n",
    "''')\n",
    "\n",
    "# Combine the fact table with all dealers ensuring all necessary records are included\n",
    "FACT_Fact = FACT_Fact.unionByName(all_dealers, allowMissingColumns=True).withColumn(\"run_date\", col(\"run_date\").cast(\"date\"))\n",
    "\n",
    "# Write the final fact table to the specified location\n",
    "FACT_Fact.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"generic_db.generic_fact_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a31377a-9c1c-4660-8b82-358ebb682bd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DIM tables transformation and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf9ab17-1677-4877-b709-35dea670966c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create and Store DIM Dealer Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create and store the DIM Dealer table with generic names\n",
    "DIM_Dealer = spark.sql('''\n",
    "  select distinct\n",
    "    date_add(d.run_date, -1)||d.dealer_id   as dealer_key,  -- Unique key for the dealer\n",
    "    date_add(d.run_date, -1)                as run_date,      -- Date of the run, adjusted by one day\n",
    "    d.country                               as country,        -- Country of the dealer\n",
    "    d.dealer_id                             as dealer_number,  -- Dealer identification number\n",
    "    d.pm_initials                           as pm,            -- Initials of the primary manager\n",
    "    d.wsa_initials                          as wsa,           -- Initials of the warranty service advisor\n",
    "    d.gm                                    as rsd,           -- General manager\n",
    "    d.dealer_full_name                      as dealer_name,    -- Full name of the dealer\n",
    "    d.state                                 as state,         -- State where the dealer is located\n",
    "    d.brand                                 as dealer_brand,   -- Brand associated with the dealer\n",
    "    d.termination_code                      as termination_code,-- Code indicating termination status\n",
    "    d.machine_terms_code                    as machine_terms_code, -- Code for machine credit terms\n",
    "    d.machine_terms_date                    as machine_terms_date, -- Date for machine credit terms\n",
    "    d.machine_terms_name                    as machine_terms_name, -- Name for machine credit terms\n",
    "    d.parts_terms_code                      as parts_terms_code,   -- Code for parts credit terms\n",
    "    d.parts_terms_date                      as parts_terms_date,   -- Date for parts credit terms\n",
    "    d.parts_terms_name                      as parts_terms_name,   -- Name for parts credit terms\n",
    "    d.entity_dealer_number                  as entity_dealer_number,-- Entity dealer number\n",
    "    d.credit_rating                         as credit_rating,      -- Credit rating of the dealer\n",
    "    cr.credit_limit_amt                     as credit_limit_amt,   -- Credit limit amount from the credit limit table\n",
    "    date_format(d.run_date, 'yyyyMM')       as fx_id,            -- Formatted date for currency exchange\n",
    "    d.brand                                 as dealer_brand2      -- Brand for dealer, potentially for further categorization\n",
    "  from analytics_db.incremental_table d\n",
    "    left join \n",
    "      (select \n",
    "        dealer_key, \n",
    "        date_add(run_date, -1) as run_date,  -- Adjusted run date for credit limit aggregation\n",
    "        sum(credit_limit_amt) as credit_limit_amt -- Total credit limit amount for the dealer\n",
    "      from analytics_db.incremental_creditlimit\n",
    "      group by dealer_key, run_date) cr \n",
    "    on d.dealer_key = cr.dealer_key and date_add(d.run_date, -1) = cr.run_date\n",
    "  where d.dealer_id is not null  -- Ensure only valid dealers are included\n",
    "''')\n",
    "\n",
    "# Write the resulting DataFrame to a table in the analytics database\n",
    "DIM_Dealer.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.dim_dealer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1691e127-386c-473d-836b-569cb604b1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create and store the DIM Dealer Behaviour table with generic names\n",
    "DIM_DealerBehaviour = spark.sql('''\n",
    "  select distinct\n",
    "    date_add(b.run_date, -1)||b.dealer_id as dealer_key,  -- Unique dealer key generated from run date and dealer ID\n",
    "    CASE \n",
    "      WHEN b.behavior_code = 'PF' THEN 'Performing'        -- Classify as Performing\n",
    "      WHEN b.behavior_code = 'PD' THEN 'Problem Dealer'    -- Classify as Problem Dealer\n",
    "      WHEN b.behavior_code = 'BD' THEN 'Bad Debt'          -- Classify as Bad Debt\n",
    "      WHEN b.behavior_code = 'CW' THEN 'Credit Watch'      -- Classify as Credit Watch\n",
    "      WHEN b.behavior_code = 'LG' THEN 'Litigation'        -- Classify as Litigation\n",
    "    END as behavior_desc,                                   -- Description of dealer behavior\n",
    "    b.problem_dealer,                                      -- Indicator for problem dealer\n",
    "    b.bad_debt,                                           -- Indicator for bad debt\n",
    "    cast(b.loss_estimate as decimal(18,2)) as loss_estimate,  -- Loss estimate cast to decimal\n",
    "    cast(b.loss_reserve as decimal(18,2)) as loss_reserve   -- Loss reserve cast to decimal\n",
    "  from analytics_db.incremental_behavior b                 -- Source table for incremental behavior data\n",
    "''')\n",
    "\n",
    "# Write the results to a new table in the analytics database\n",
    "DIM_DealerBehaviour.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.dim_dealerbehavior\")  -- Save as a new table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a10157-d9aa-4c93-94c5-6f5a6e5e2dab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create and Store DIM Equipment Table"
    }
   },
   "outputs": [],
   "source": [
    "# Create and store the DIM Equipment table with generic names\n",
    "DIM_Equipment = spark.sql('''\n",
    "  WITH EQUIP AS\n",
    "    (SELECT distinct \n",
    "        e.*,\n",
    "        coalesce(p.gamma_desc, p2.gamma_desc) as gamma_desc,\n",
    "        coalesce(p.industry_class_nm, p2.industry_class_nm) as industry_class_nm,\n",
    "        coalesce(p.product_desc, p2.product_desc) as product_desc,\n",
    "        coalesce(p.category_nm, p2.category_nm) as category_nm\n",
    "    from analytics_db.incremental_equipment e\n",
    "    left join analytics_db.incremental_product p\n",
    "        on e.tech_type = p.tech_type \n",
    "        and e.run_date = p.run_date\n",
    "    left join analytics_db.incremental_product as p2 \n",
    "        on trim(e.price_book||e.series||e.model) = trim(p2.pb_code||p2.series||p2.model) \n",
    "        and e.run_date = p2.run_date\n",
    "    ) \n",
    "  select distinct\n",
    "    date_add(e.run_date, -1)||e.equipment_id    as equipment_key,  -- Unique key for equipment\n",
    "    date_add(e.run_date, -1)                    as run_date,         -- Run date for the equipment\n",
    "    trim(e.price_book||e.series||e.model)       as pbsm_id,         -- Price book, series, and model ID\n",
    "    e.serial_number                            as serial_number,     -- Serial number of the equipment\n",
    "    e.manufacturer_code                        as manufacturer_code,  -- Manufacturer code\n",
    "    e.current_status_code                      as current_status_code, -- Current status of the equipment\n",
    "    e.used_equipment_product_code              as used_equipment_product_code, -- Used equipment product code\n",
    "    e.gamma_desc                               as gamma_desc,        -- Gamma description\n",
    "    e.industry_class_nm                        as industry_class,     -- Industry class name\n",
    "    CASE WHEN isnull(e.product_desc) or trim(e.product_desc) = '' THEN 'MISCELLANEOUS' ELSE e.product_desc END as product_desc, -- Product description\n",
    "    CASE WHEN isnull(e.category_nm) or trim(e.category_nm) = '' THEN 'Miscellaneous' ELSE e.category_nm END as category_name -- Category name\n",
    "  from EQUIP e\n",
    "    inner join analytics_db.complete_fact r \n",
    "      on e.equipment_id = r.equipment_id and date_add(e.run_date, -1) = r.run_date\n",
    "''')\n",
    "\n",
    "DIM_Equipment.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.dim_equipment\")  -- Save the DIM Equipment table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "677d40cb-c23f-4e79-9423-bcfa7c2b08bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create and store the DIM Dates table with generic names\n",
    "DIM_Dates = spark.sql('''\n",
    "  select distinct\n",
    "    run_date,          -- The date of the run\n",
    "    run_month,         -- The month of the run\n",
    "    run_month_id,      -- The identifier for the run month\n",
    "    year_month,        -- The year and month combined\n",
    "    run_year           -- The year of the run\n",
    "  from analytics_db.complete_fact   -- Source table for the data\n",
    "''')\n",
    "\n",
    "# Write the DIM_Dates DataFrame to a table with overwrite mode\n",
    "DIM_Dates.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.dim_dates\")  -- Target table for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ffe4f1-db72-438d-a53c-552f28f8402d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create and store the DIM Receivables table with generic names\n",
    "DIM_Receivables = spark.sql('''\n",
    "  select distinct\n",
    "    fact_id||run_date as key_fact,          -- Unique identifier for the fact\n",
    "    bearing_type,                           -- Type of bearing\n",
    "    product_brand,                          -- Brand of the product\n",
    "    country,                                -- Country of the transaction\n",
    "    new_or_used_indicator,                  -- Indicator for new or used item\n",
    "    receivable_date,                        -- Date of the receivable\n",
    "    owner,                                  -- Owner of the receivable\n",
    "    ag_percentage,                          -- AG percentage\n",
    "    ce_percentage,                          -- CE percentage\n",
    "    last_interest_date,                     -- Last interest date\n",
    "    current_interest_date,                  -- Current interest date\n",
    "    current_maturity_date,                  -- Current maturity date\n",
    "    maturity_date,                          -- Maturity date of the receivable\n",
    "    interest_bearing_code,                  -- Code for interest bearing\n",
    "    suffix,                                 -- Suffix for the receivable\n",
    "    aging_comparison,                       -- Aging comparison flag\n",
    "    past_due_days,                         -- Number of past due days\n",
    "    rounded_past_due_days,                 -- Rounded number of past due days\n",
    "    dealer_credits,                         -- Dealer credits flag\n",
    "    aging_buckets,                          -- Aging buckets\n",
    "    short_aging_buckets,                    -- Short aging buckets\n",
    "    product_ind,                           -- Product indicator\n",
    "    captive_allied,                         -- Captive allied flag\n",
    "    new_or_used_desc,                       -- Description for new or used\n",
    "    currency                                -- Currency of the transaction  \n",
    "  from analytics_db.complete_fact           -- Source table for the data\n",
    "''')\n",
    "DIM_Receivables.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\"analytics_db.dim_receivables\")  # Save the table with overwrite mode"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3821267969195405,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "(Clone) NA Commercial Lending ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
